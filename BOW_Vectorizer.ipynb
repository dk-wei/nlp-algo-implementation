{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BOW Vectorizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNErfPfiB6Bkz5n9f+0qZtS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dk-wei/nlp-algo-implementation/blob/main/BOW_Vectorizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVdHO_y2jgT1"
      },
      "source": [
        "# `CountVectorizer`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmciN3yPhl_w",
        "outputId": "7f160498-b9a1-4cf6-a523-7c1efa345060"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "  \n",
        "document = [\"One Geek helps Two Geeks\",\n",
        "            \"Two Geeks help Four Geeks at GeeksforGeeks\",\n",
        "            \"Each Geek helps many other Geeks at GeeksforGeeks\"]\n",
        "  \n",
        "# Create a Vectorizer Object\n",
        "vectorizer = CountVectorizer(binary = True,   #加了binary之后，就没有frequency了，只有出现和不出现\n",
        "                             ngram_range=(1, 2),  #unigram, bigram...ngram\n",
        "                             lowercase=True,\n",
        "                             dtype=np.int32\n",
        "                             )\n",
        "\n",
        "vectorizer.fit(document)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
              "                dtype=<class 'numpy.int32'>, encoding='utf-8', input='content',\n",
              "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
              "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_OaCtC2upvn",
        "outputId": "e2468834-7145-48ab-9f10-ecdd3d49fe3e"
      },
      "source": [
        "# summarize\n",
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['at', 'at geeksforgeeks', 'each', 'each geek', 'four', 'four geeks', 'geek', 'geek helps', 'geeks', 'geeks at', 'geeks help', 'geeksforgeeks', 'help', 'help four', 'helps', 'helps many', 'helps two', 'many', 'many other', 'one', 'one geek', 'other', 'other geeks', 'two', 'two geeks']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfFhk-HKsmSu",
        "outputId": "0425972d-c049-4a51-f12d-c8cc5efffafb"
      },
      "source": [
        "# Printing the identified Unique words along with their indices\n",
        "print(\"Vocabulary: \", vectorizer.vocabulary_)\n",
        "  \n",
        "# Encode the Document\n",
        "vector = vectorizer.transform(document)\n",
        "  \n",
        "# Summarizing the Encoded Texts\n",
        "print(\"Encoded Document is:\")\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary:  {'one': 19, 'geek': 6, 'helps': 14, 'two': 23, 'geeks': 8, 'one geek': 20, 'geek helps': 7, 'helps two': 16, 'two geeks': 24, 'help': 12, 'four': 4, 'at': 0, 'geeksforgeeks': 11, 'geeks help': 10, 'help four': 13, 'four geeks': 5, 'geeks at': 9, 'at geeksforgeeks': 1, 'each': 2, 'many': 17, 'other': 21, 'each geek': 3, 'helps many': 15, 'many other': 18, 'other geeks': 22}\n",
            "Encoded Document is:\n",
            "(3, 25)\n",
            "[[0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1]\n",
            " [1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1]\n",
            " [1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U67oGal1jK8b",
        "outputId": "3df07374-4cf1-4d91-e7f3-f766b277fc83"
      },
      "source": [
        "# encode another new document\n",
        "text2 = [\"the puppy is a Geek\"]\n",
        "vector = vectorizer.transform(text2)\n",
        "\n",
        "print(vector.toarray())"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjAnk0IEjeM_"
      },
      "source": [
        "可以看到，很遗憾只有geek在vocabulary中，这也是CountVectorizer很大一个弊端，未出现的token就fit不出来。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ti5qSDo3k59M"
      },
      "source": [
        ""
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4QWN3xflHUc"
      },
      "source": [
        "# `TfidfVectorizer`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRw88J_xk56S",
        "outputId": "402903c1-744b-4b6a-cfe1-f3def13d4acf"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# list of text documents\n",
        "text = [\"One Geek helps Two Geeks\",\n",
        "            \"Two Geeks help Four Geeks\",\n",
        "            \"Each Geek helps many other Geeks at GeeksforGeeks\"]\n",
        "# create the transform\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2),\n",
        "                             smooth_idf=True,\n",
        "                             use_idf=True,\n",
        "                             lowercase=True,\n",
        "                             dtype=np.int32\n",
        "                             #binary = True\n",
        "                             )\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(text)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:1817: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. <class 'numpy.int32'> 'dtype' will be converted to np.float64.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.int32'>, encoding='utf-8', input='content',\n",
              "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
              "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, use_idf=True, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bptlZ2vruPJx",
        "outputId": "fb3919ea-55a1-4b75-b2f1-04dccb4226d7"
      },
      "source": [
        "# summarize\n",
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['at', 'at geeksforgeeks', 'each', 'each geek', 'four', 'four geeks', 'geek', 'geek helps', 'geeks', 'geeks at', 'geeks help', 'geeksforgeeks', 'help', 'help four', 'helps', 'helps many', 'helps two', 'many', 'many other', 'one', 'one geek', 'other', 'other geeks', 'two', 'two geeks']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQV0to6Hk5kH",
        "outputId": "30ffbd74-f079-4af5-fc68-2c67f51e06a4"
      },
      "source": [
        "# summarize\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'one': 19, 'geek': 6, 'helps': 14, 'two': 23, 'geeks': 8, 'one geek': 20, 'geek helps': 7, 'helps two': 16, 'two geeks': 24, 'help': 12, 'four': 4, 'geeks help': 10, 'help four': 13, 'four geeks': 5, 'each': 2, 'many': 17, 'other': 21, 'at': 0, 'geeksforgeeks': 11, 'each geek': 3, 'helps many': 15, 'many other': 18, 'other geeks': 22, 'geeks at': 9, 'at geeksforgeeks': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJgHLLzOnLZv",
        "outputId": "329ee163-f8d0-42d9-a915-8e210d1cca5d"
      },
      "source": [
        "print(vectorizer.idf_)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.28768207 1.28768207 1.         1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.28768207 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.28768207\n",
            " 1.28768207]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTonAP0rlKc0",
        "outputId": "69932ba3-9eef-42f8-b376-f2d5785df38b"
      },
      "source": [
        "# encode document\n",
        "vector = vectorizer.transform(text)\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 25)\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.30443385 0.30443385 0.23642005 0.         0.         0.\n",
            "  0.         0.         0.30443385 0.         0.40029393 0.\n",
            "  0.         0.40029393 0.40029393 0.         0.         0.30443385\n",
            "  0.30443385]\n",
            " [0.         0.         0.         0.         0.36388646 0.36388646\n",
            "  0.         0.         0.42983441 0.         0.36388646 0.\n",
            "  0.36388646 0.36388646 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.27674503\n",
            "  0.27674503]\n",
            " [0.27645809 0.27645809 0.27645809 0.27645809 0.         0.\n",
            "  0.2102535  0.2102535  0.1632806  0.27645809 0.         0.27645809\n",
            "  0.         0.         0.2102535  0.27645809 0.         0.27645809\n",
            "  0.27645809 0.         0.         0.27645809 0.27645809 0.\n",
            "  0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVvcQltRoLvB"
      },
      "source": [
        "不同的doc/sent中，每个token的位置出现的不是freq，而是tf-idf值"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L50eIYdNlKZK",
        "outputId": "20ff5370-5a4c-4008-c4ce-ad522bf6811f"
      },
      "source": [
        "# encode another new document\n",
        "text2 = [\"the puppy is a Geek Geek at GeeksforGeeks\"]\n",
        "vector = vectorizer.transform(text2)\n",
        "print(vector.toarray())"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.43381609 0.43381609 0.         0.         0.         0.\n",
            "  0.65985664 0.         0.         0.         0.         0.43381609\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoQ1PfA7pfMC"
      },
      "source": [
        "同样的问题也是new token的值为0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7RjJYxIsX6d"
      },
      "source": [
        ""
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T86W8Jt2llEO"
      },
      "source": [
        "# `HashingVectorizer`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_OYo4sRlKTN"
      },
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "# list of text documents\n",
        "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
        "# create the transform\n",
        "vectorizer = HashingVectorizer(n_features=20)\n",
        "# encode document\n",
        "vector = vectorizer.transform(text)"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Isn9tEy2lKRR",
        "outputId": "0d78f40f-3101-4c44-990f-b1668f1dc6a2"
      },
      "source": [
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 20)\n",
            "[[ 0.          0.          0.          0.          0.          0.33333333\n",
            "   0.         -0.33333333  0.33333333  0.          0.          0.33333333\n",
            "   0.          0.          0.         -0.33333333  0.          0.\n",
            "  -0.66666667  0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2ULh765lKNt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wzAxRq-lKLf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnedsmqDlKIG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62jbnqQ1lKE4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHQr8JujlKCS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0yValo_lJ-_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRFxlX62k5g9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjYCmmWCk5eX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdM2_2wtk5bJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlEzd6Ihk5YM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWi_0JiLk0aj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}