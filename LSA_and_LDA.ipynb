{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSA and LDA.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNRane11QGo35h2Wzz63cAn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dk-wei/nlp-algo-implementation/blob/main/LSA_and_LDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dtm6DSKZe4tQ"
      },
      "source": [
        "Before the state-of-the-art word embedding technique, Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA) area good approaches to deal with NLP problems. Both LSA and LDA have same input which is **Bag of words in matrix format**. LSA focus on **reducing matrix dimension** while LDA solves **topic modeling** problems.\n",
        "\n",
        "I will not go through mathematical detail and as there is lot of great material for that. You may check it from reference. For the sake of keeping it easy to understand, I did not do pre-processing such as stopwords removal. It is critical part when you use LSA, LSI and LDA. After reading this article, you will know:\n",
        " - Latent Semantic Analysis (LSA)\n",
        " - Latent Dirichlet Allocation (LDA)\n",
        " - Take Away\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mt1p9r13dHCy",
        "outputId": "8b888250-3636-4045-d7ec-5929e05410cb"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "train_raw = fetch_20newsgroups(subset='train')\n",
        "test_raw = fetch_20newsgroups(subset='test')\n",
        "\n",
        "x_train = train_raw.data\n",
        "y_train = train_raw.target\n",
        "\n",
        "x_test = test_raw.data\n",
        "y_test = test_raw.target"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koMuVAXrfcgd"
      },
      "source": [
        "# Latent Semantic Analysis (LSA)\n",
        "\n",
        "LSA for natural language processing task was introduced by Jerome Bellegarda in 2005. The objective of LSA is reducing dimension for classification. The idea is that words will occurs in similar pieces of text if they have similar meaning. We usually use Latent Semantic Indexing (LSI) as an alternative name in NLP field.\n",
        "\n",
        "First of all, we have m documents and n words as input. An m * n matrix can be constructed while column and row are document and word respectively. You can use count occurrence or TF-IDF score. However, TF-IDF is better than count occurrence in most of the time as high frequency do not account for better classification.\n",
        "\n",
        "![](https://miro.medium.com/proxy/0*7r2GKRepjh5Fl41r.png)\n",
        "\n",
        "The idea of TF-IDF is that high frequency may not able to provide much information gain. In another word, rare words contribute more weights to the model. Word importance will be increased if the number of occurrence within same document (i.e. training record). On the other hand, it will be decreased if it occurs in corpus (i.e. other training records). For detail, you may check this blog.\n",
        "\n",
        "The challenge is that the matrix is very sparse (or high dimension) and noisy (or include lots of low frequency word). So truncated SVD is adopted to reduce dimension.\n",
        "\n",
        "![](https://miro.medium.com/max/1400/1*Z0EUVs7QElEqRqXtqut_FQ.png)\n",
        "\n",
        "The idea of SVD is finding the most valuable information and using lower dimension t to represent same thing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fTEyBysdvcu",
        "outputId": "b6ebcf0d-e184-43ed-9de4-5ec28378ea57"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "def build_lsa(x_train, x_test, dim=50):\n",
        "    tfidf_vec = TfidfVectorizer(use_idf=True, norm='l2')\n",
        "    svd = TruncatedSVD(n_components=dim)\n",
        "    \n",
        "    transformed_x_train = tfidf_vec.fit_transform(x_train)\n",
        "    transformed_x_test = tfidf_vec.transform(x_test)\n",
        "    \n",
        "    print('TF-IDF output shape:', transformed_x_train.shape)\n",
        "    \n",
        "    x_train_svd = svd.fit_transform(transformed_x_train)\n",
        "    x_test_svd = svd.transform(transformed_x_test)\n",
        "    \n",
        "    print('LSA output shape:', x_train_svd.shape)\n",
        "    \n",
        "    explained_variance = svd.explained_variance_ratio_.sum()\n",
        "    print(\"Sum of explained variance ratio: %d%%\" % (int(explained_variance * 100)))\n",
        "    \n",
        "    return tfidf_vec, svd, x_train_svd, x_test_svd\n",
        "\n",
        "\n",
        "tfidf_vec, svd, x_train_lda, x_test_lda = build_lsa(x_train, x_test)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF-IDF output shape: (11314, 130107)\n",
            "LSA output shape: (11314, 50)\n",
            "Sum of explained variance ratio: 8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NK7CG_-d0SG",
        "outputId": "2cbabcda-4e69-4190-b60e-c6f982153684"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "\n",
        "lr_model = LogisticRegression(solver='newton-cg',n_jobs=-1)\n",
        "lr_model.fit(x_train_lda, y_train)\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True)\n",
        "    \n",
        "scores = cross_val_score(lr_model, x_test_lda, y_test, cv=cv, scoring='accuracy')\n",
        "print(\"Accuracy: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6564 (+/- 0.0431)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaWPQ98CfxMk"
      },
      "source": [
        "# Latent Dirichlet Allocation (LDA)\n",
        "\n",
        "LDA is introduced by David Blei, Andrew Ng and Michael O. Jordan in 2003. It is unsupervised learning and topic model is the typical example. The assumption is that each document mix with various topics and every topic mix with various words.\n",
        "![](https://miro.medium.com/max/1138/1*SHahRtoGw3JP48e806DsIw.png)\n",
        "\n",
        "Intuitively, you can image that we have two layer of aggregations. First layer is the distribution of categories. For example, we have finance news, weather news and political news. Second layer is distribution of words within the category. For instance, we can find “sunny” and “cloud” in weather news while “money” and “stock” exists in finance news.\n",
        "\n",
        "However, “a”, “with” and “can” do not contribute on topic modeling problem. Those words exist among documents and will have roughly same probability between categories. Therefore, stopwords removal is a critical step to achieve a better result.\n",
        "\n",
        "![](https://miro.medium.com/max/1270/1*qwA4jyRFBB6Htn3X4aftSw.png)\n",
        "\n",
        "For particular document d, we get the topic distribution which is θ. From this distribution(θ), topic t will be chosen and selecting corresponding word from ϕ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dH3q7R_2d4Qi",
        "outputId": "7d198b06-8aa4-4017-fdda-4421551f85d4"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "def build_lda(x_train, num_of_topic=10):\n",
        "    vec = CountVectorizer()\n",
        "    transformed_x_train = vec.fit_transform(x_train)\n",
        "    feature_names = vec.get_feature_names()\n",
        "\n",
        "    lda = LatentDirichletAllocation(\n",
        "        n_components=num_of_topic, max_iter=5, \n",
        "        learning_method='online', random_state=0)\n",
        "    lda.fit(transformed_x_train)\n",
        "\n",
        "    return lda, vec, feature_names\n",
        "\n",
        "def display_word_distribution(model, feature_names, n_word):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(\"Topic %d:\" % (topic_idx))\n",
        "        words = []\n",
        "        for i in topic.argsort()[:-n_word - 1:-1]:\n",
        "            words.append(feature_names[i])\n",
        "        print(words)\n",
        "\n",
        "lda_model, vec, feature_names = build_lda(x_train)\n",
        "display_word_distribution(\n",
        "    model=lda_model, feature_names=feature_names, \n",
        "    n_word=5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic 0:\n",
            "['the', 'for', 'and', 'to', 'edu']\n",
            "Topic 1:\n",
            "['c_', 'w7', 'hz', 'mv', 'ck']\n",
            "Topic 2:\n",
            "['space', 'nasa', 'cmu', 'science', 'edu']\n",
            "Topic 3:\n",
            "['the', 'to', 'of', 'for', 'and']\n",
            "Topic 4:\n",
            "['the', 'to', 'of', 'and', 'in']\n",
            "Topic 5:\n",
            "['the', 'of', 'and', 'in', 'were']\n",
            "Topic 6:\n",
            "['edu', 'team', 'he', 'game', '10']\n",
            "Topic 7:\n",
            "['ax', 'max', 'g9v', 'b8f', 'a86']\n",
            "Topic 8:\n",
            "['db', 'bike', 'ac', 'image', 'dod']\n",
            "Topic 9:\n",
            "['nec', 'mil', 'navy', 'sg', 'behanna']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyEy_ldsgKSK"
      },
      "source": [
        "# Take Away\n",
        "\n",
        "- Both of them use **Bag-of-words** as input matrix\n",
        "- The challenge of SVD is that we are **hard to determine the optimal number of dimension**. In general, low dimension consume less resource but we may not able to distinguish opposite meaning words while high dimension overcome it but consuming more resource.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRcq4M3Med51"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}