{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSA and LDA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNG0KKprKDophpJ3Mv/unm8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dk-wei/nlp-algo-implementation/blob/main/LSA_and_LDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dtm6DSKZe4tQ"
      },
      "source": [
        "Before the state-of-the-art word embedding technique, Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA) area good approaches to deal with NLP problems. Both LSA and LDA have same input which is **Bag of words in matrix format**. LSA focus on **reducing matrix dimension** while LDA solves **topic modeling** problems.\n",
        "\n",
        "I will not go through mathematical detail and as there is lot of great material for that. You may check it from reference. For the sake of keeping it easy to understand, I did not do pre-processing such as stopwords removal. It is critical part when you use LSA, LSI and LDA. After reading this article, you will know:\n",
        " - Latent Semantic Analysis (LSA)\n",
        " - Latent Dirichlet Allocation (LDA)\n",
        " - Take Away\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mt1p9r13dHCy",
        "outputId": "8b888250-3636-4045-d7ec-5929e05410cb"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "train_raw = fetch_20newsgroups(subset='train')\n",
        "test_raw = fetch_20newsgroups(subset='test')\n",
        "\n",
        "x_train = train_raw.data\n",
        "y_train = train_raw.target\n",
        "\n",
        "x_test = test_raw.data\n",
        "y_test = test_raw.target"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koMuVAXrfcgd"
      },
      "source": [
        "# Latent Semantic Analysis (LSA)\n",
        "\n",
        "LSA for natural language processing task was introduced by Jerome Bellegarda in 2005. The objective of LSA is reducing dimension for classification. The idea is that words will occurs in similar pieces of text if they have similar meaning. We usually use Latent Semantic Indexing (LSI) as an alternative name in NLP field.\n",
        "\n",
        "First of all, we have m documents and n words as input. An m * n matrix can be constructed while column and row are document and word respectively. You can use count occurrence or TF-IDF score. However, TF-IDF is better than count occurrence in most of the time as high frequency do not account for better classification.\n",
        "\n",
        "![](https://miro.medium.com/proxy/0*7r2GKRepjh5Fl41r.png)\n",
        "\n",
        "The idea of TF-IDF is that high frequency may not able to provide much information gain. In another word, rare words contribute more weights to the model. Word importance will be increased if the number of occurrence within same document (i.e. training record). On the other hand, it will be decreased if it occurs in corpus (i.e. other training records). For detail, you may check this blog.\n",
        "\n",
        "The challenge is that the matrix is very sparse (or high dimension) and noisy (or include lots of low frequency word). So truncated SVD is adopted to reduce dimension.\n",
        "\n",
        "![](https://miro.medium.com/max/1400/1*Z0EUVs7QElEqRqXtqut_FQ.png)\n",
        "\n",
        "**The idea of SVD is finding the most valuable information and using lower dimension t to represent same thing.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fTEyBysdvcu",
        "outputId": "b6ebcf0d-e184-43ed-9de4-5ec28378ea57"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "def build_lsa(x_train, x_test, dim=50):\n",
        "    tfidf_vec = TfidfVectorizer(use_idf=True, norm='l2')\n",
        "    svd = TruncatedSVD(n_components=dim)\n",
        "    \n",
        "    transformed_x_train = tfidf_vec.fit_transform(x_train)\n",
        "    transformed_x_test = tfidf_vec.transform(x_test)\n",
        "    \n",
        "    print('TF-IDF output shape:', transformed_x_train.shape)\n",
        "    \n",
        "    x_train_svd = svd.fit_transform(transformed_x_train)\n",
        "    x_test_svd = svd.transform(transformed_x_test)\n",
        "    \n",
        "    print('LSA output shape:', x_train_svd.shape)\n",
        "    \n",
        "    explained_variance = svd.explained_variance_ratio_.sum()\n",
        "    print(\"Sum of explained variance ratio: %d%%\" % (int(explained_variance * 100)))\n",
        "    \n",
        "    return tfidf_vec, svd, x_train_svd, x_test_svd\n",
        "\n",
        "\n",
        "tfidf_vec, svd, x_train_lda, x_test_lda = build_lsa(x_train, x_test)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF-IDF output shape: (11314, 130107)\n",
            "LSA output shape: (11314, 50)\n",
            "Sum of explained variance ratio: 8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daljlxF-ipr3",
        "outputId": "22650388-fcca-4d9c-ec6c-a4fb487e51d6"
      },
      "source": [
        "x_train[:5]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\",\n",
              " \"From: guykuo@carson.u.washington.edu (Guy Kuo)\\nSubject: SI Clock Poll - Final Call\\nSummary: Final call for SI clock reports\\nKeywords: SI,acceleration,clock,upgrade\\nArticle-I.D.: shelley.1qvfo9INNc3s\\nOrganization: University of Washington\\nLines: 11\\nNNTP-Posting-Host: carson.u.washington.edu\\n\\nA fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\\n\\nGuy Kuo <guykuo@u.washington.edu>\\n\",\n",
              " 'From: twillis@ec.ecn.purdue.edu (Thomas E Willis)\\nSubject: PB questions...\\nOrganization: Purdue University Engineering Computer Network\\nDistribution: usa\\nLines: 36\\n\\nwell folks, my mac plus finally gave up the ghost this weekend after\\nstarting life as a 512k way back in 1985.  sooo, i\\'m in the market for a\\nnew machine a bit sooner than i intended to be...\\n\\ni\\'m looking into picking up a powerbook 160 or maybe 180 and have a bunch\\nof questions that (hopefully) somebody can answer:\\n\\n* does anybody know any dirt on when the next round of powerbook\\nintroductions are expected?  i\\'d heard the 185c was supposed to make an\\nappearence \"this summer\" but haven\\'t heard anymore on it - and since i\\ndon\\'t have access to macleak, i was wondering if anybody out there had\\nmore info...\\n\\n* has anybody heard rumors about price drops to the powerbook line like the\\nones the duo\\'s just went through recently?\\n\\n* what\\'s the impression of the display on the 180?  i could probably swing\\na 180 if i got the 80Mb disk rather than the 120, but i don\\'t really have\\na feel for how much \"better\" the display is (yea, it looks great in the\\nstore, but is that all \"wow\" or is it really that good?).  could i solicit\\nsome opinions of people who use the 160 and 180 day-to-day on if its worth\\ntaking the disk size and money hit to get the active display?  (i realize\\nthis is a real subjective question, but i\\'ve only played around with the\\nmachines in a computer store breifly and figured the opinions of somebody\\nwho actually uses the machine daily might prove helpful).\\n\\n* how well does hellcats perform?  ;)\\n\\nthanks a bunch in advance for any info - if you could email, i\\'ll post a\\nsummary (news reading time is at a premium with finals just around the\\ncorner... :( )\\n--\\nTom Willis  \\\\  twillis@ecn.purdue.edu    \\\\    Purdue Electrical Engineering\\n---------------------------------------------------------------------------\\n\"Convictions are more dangerous enemies of truth than lies.\"  - F. W.\\nNietzsche\\n',\n",
              " 'From: jgreen@amber (Joe Green)\\nSubject: Re: Weitek P9000 ?\\nOrganization: Harris Computer Systems Division\\nLines: 14\\nDistribution: world\\nNNTP-Posting-Host: amber.ssd.csd.harris.com\\nX-Newsreader: TIN [version 1.1 PL9]\\n\\nRobert J.C. Kyanko (rob@rjck.UUCP) wrote:\\n> abraxis@iastate.edu writes in article <abraxis.734340159@class1.iastate.edu>:\\n> > Anyone know about the Weitek P9000 graphics chip?\\n> As far as the low-level stuff goes, it looks pretty nice.  It\\'s got this\\n> quadrilateral fill command that requires just the four points.\\n\\nDo you have Weitek\\'s address/phone number?  I\\'d like to get some information\\nabout this chip.\\n\\n--\\nJoe Green\\t\\t\\t\\tHarris Corporation\\njgreen@csd.harris.com\\t\\t\\tComputer Systems Division\\n\"The only thing that really scares me is a person with no sense of humor.\"\\n\\t\\t\\t\\t\\t\\t-- Jonathan Winters\\n',\n",
              " 'From: jcm@head-cfa.harvard.edu (Jonathan McDowell)\\nSubject: Re: Shuttle Launch Question\\nOrganization: Smithsonian Astrophysical Observatory, Cambridge, MA,  USA\\nDistribution: sci\\nLines: 23\\n\\nFrom article <C5owCB.n3p@world.std.com>, by tombaker@world.std.com (Tom A Baker):\\n>>In article <C5JLwx.4H9.1@cs.cmu.edu>, ETRAT@ttacs1.ttu.edu (Pack Rat) writes...\\n>>>\"Clear caution & warning memory.  Verify no unexpected\\n>>>errors. ...\".  I am wondering what an \"expected error\" might\\n>>>be.  Sorry if this is a really dumb question, but\\n> \\n> Parity errors in memory or previously known conditions that were waivered.\\n>    \"Yes that is an error, but we already knew about it\"\\n> I\\'d be curious as to what the real meaning of the quote is.\\n> \\n> tom\\n\\n\\nMy understanding is that the \\'expected errors\\' are basically\\nknown bugs in the warning system software - things are checked\\nthat don\\'t have the right values in yet because they aren\\'t\\nset till after launch, and suchlike. Rather than fix the code\\nand possibly introduce new bugs, they just tell the crew\\n\\'ok, if you see a warning no. 213 before liftoff, ignore it\\'.\\n\\n - Jonathan\\n\\n\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSznOaLTiwh6",
        "outputId": "caff0bc3-1318-4ce3-99a5-cb07ca7ac982"
      },
      "source": [
        "x_train_lda[:2]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.24972705, -0.06942864, -0.01311425, -0.05103983, -0.00403584,\n",
              "        -0.0069866 , -0.0281033 , -0.03696149, -0.00102237, -0.01865511,\n",
              "         0.00171095,  0.0094233 , -0.09034356, -0.01437922, -0.05764015,\n",
              "        -0.00149756, -0.05819083,  0.06553346, -0.01240388, -0.06645535,\n",
              "         0.07101369,  0.01199455,  0.0445082 ,  0.03394705, -0.04561099,\n",
              "        -0.05714162, -0.03763445,  0.02101698, -0.01736399,  0.01639008,\n",
              "         0.04381269, -0.01933203, -0.00101538,  0.03542597, -0.02547384,\n",
              "         0.00398878,  0.0591342 ,  0.01701092, -0.0163664 , -0.08605762,\n",
              "         0.00710651, -0.05365106,  0.01285126, -0.04646758,  0.04073939,\n",
              "         0.04262537,  0.0565492 ,  0.08231343,  0.01841167, -0.03029937],\n",
              "       [ 0.1399918 , -0.07671644, -0.03975164, -0.02232068,  0.01438013,\n",
              "         0.03615991, -0.03166721, -0.01104911, -0.03521134,  0.01150632,\n",
              "        -0.02008854,  0.01800078,  0.03558538,  0.00249563, -0.02585091,\n",
              "        -0.01435933,  0.00067577, -0.00648094, -0.00753178, -0.03406904,\n",
              "         0.03238867, -0.01818821, -0.00332201,  0.01463326, -0.00436262,\n",
              "         0.00169928, -0.00385545, -0.00631233,  0.01843858,  0.01199998,\n",
              "        -0.01724246, -0.0259694 ,  0.002556  , -0.00720848,  0.00895311,\n",
              "         0.0106214 , -0.01247758, -0.00998304, -0.03606217,  0.01705046,\n",
              "        -0.00640468,  0.00945692, -0.02147574, -0.04151215,  0.05852964,\n",
              "        -0.01754242, -0.00776636,  0.04901335,  0.01243522,  0.05223295]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NK7CG_-d0SG",
        "outputId": "2cbabcda-4e69-4190-b60e-c6f982153684"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "\n",
        "lr_model = LogisticRegression(solver='newton-cg',n_jobs=-1)\n",
        "lr_model.fit(x_train_lda, y_train)\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True)\n",
        "    \n",
        "scores = cross_val_score(lr_model, x_test_lda, y_test, cv=cv, scoring='accuracy')\n",
        "print(\"Accuracy: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6564 (+/- 0.0431)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaWPQ98CfxMk"
      },
      "source": [
        "# Latent Dirichlet Allocation (LDA)\n",
        "\n",
        "LDA is introduced by David Blei, Andrew Ng and Michael O. Jordan in 2003. It is unsupervised learning and topic model is the typical example. The assumption is that each document mix with various topics and every topic mix with various words.\n",
        "![](https://miro.medium.com/max/1138/1*SHahRtoGw3JP48e806DsIw.png)\n",
        "\n",
        "Intuitively, you can image that we have two layer of aggregations. First layer is the distribution of categories. For example, we have finance news, weather news and political news. Second layer is distribution of words within the category. For instance, we can find “sunny” and “cloud” in weather news while “money” and “stock” exists in finance news.\n",
        "\n",
        "However, “a”, “with” and “can” do not contribute on topic modeling problem. Those words exist among documents and will have roughly same probability between categories. Therefore, stopwords removal is a critical step to achieve a better result.\n",
        "\n",
        "![](https://miro.medium.com/max/1270/1*qwA4jyRFBB6Htn3X4aftSw.png)\n",
        "\n",
        "For particular document d, we get the topic distribution which is θ. From this distribution(θ), topic t will be chosen and selecting corresponding word from ϕ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dH3q7R_2d4Qi",
        "outputId": "7d198b06-8aa4-4017-fdda-4421551f85d4"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "def build_lda(x_train, num_of_topic=10):\n",
        "    vec = CountVectorizer()\n",
        "    transformed_x_train = vec.fit_transform(x_train)\n",
        "    feature_names = vec.get_feature_names()\n",
        "\n",
        "    lda = LatentDirichletAllocation(\n",
        "        n_components=num_of_topic, max_iter=5, \n",
        "        learning_method='online', random_state=0)\n",
        "    lda.fit(transformed_x_train)\n",
        "\n",
        "    return lda, vec, feature_names\n",
        "\n",
        "def display_word_distribution(model, feature_names, n_word):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(\"Topic %d:\" % (topic_idx))\n",
        "        words = []\n",
        "        for i in topic.argsort()[:-n_word - 1:-1]:\n",
        "            words.append(feature_names[i])\n",
        "        print(words)\n",
        "\n",
        "lda_model, vec, feature_names = build_lda(x_train)\n",
        "display_word_distribution(\n",
        "    model=lda_model, feature_names=feature_names, \n",
        "    n_word=5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic 0:\n",
            "['the', 'for', 'and', 'to', 'edu']\n",
            "Topic 1:\n",
            "['c_', 'w7', 'hz', 'mv', 'ck']\n",
            "Topic 2:\n",
            "['space', 'nasa', 'cmu', 'science', 'edu']\n",
            "Topic 3:\n",
            "['the', 'to', 'of', 'for', 'and']\n",
            "Topic 4:\n",
            "['the', 'to', 'of', 'and', 'in']\n",
            "Topic 5:\n",
            "['the', 'of', 'and', 'in', 'were']\n",
            "Topic 6:\n",
            "['edu', 'team', 'he', 'game', '10']\n",
            "Topic 7:\n",
            "['ax', 'max', 'g9v', 'b8f', 'a86']\n",
            "Topic 8:\n",
            "['db', 'bike', 'ac', 'image', 'dod']\n",
            "Topic 9:\n",
            "['nec', 'mil', 'navy', 'sg', 'behanna']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyEy_ldsgKSK"
      },
      "source": [
        "# Take Away\n",
        "\n",
        "- Both of them use **Bag-of-words** as input matrix\n",
        "- The challenge of SVD is that we are **hard to determine the optimal number of dimension**. In general, low dimension consume less resource but we may not able to distinguish opposite meaning words while high dimension overcome it but consuming more resource.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7F0qqERnPwN"
      },
      "source": [
        "##  附 Truncated SVD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRcq4M3Med51",
        "outputId": "5fd46031-92c6-430c-b97f-971df562a739"
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "from scipy.sparse import random as sparse_random\n",
        "X = sparse_random(100, 100, density=0.01, format='csr',\n",
        "                  random_state=42)\n",
        "\n",
        "X.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh_oUy2enu8x",
        "outputId": "ce793268-212a-4763-c43e-97c033d59661"
      },
      "source": [
        "svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
        "svd.fit(X)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TruncatedSVD(algorithm='randomized', n_components=5, n_iter=7, random_state=42,\n",
              "             tol=0.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKwuHnsroKFr",
        "outputId": "aefd20e5-db37-49e1-d9e8-5d2cfdb9f42e"
      },
      "source": [
        "print(svd.explained_variance_ratio_)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.0646051  0.06339479 0.06394407 0.05352903 0.04062679]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Fjs_sFFngam",
        "outputId": "d90113f3-89d4-4d49-b81d-b791e4e9ffb3"
      },
      "source": [
        "print(svd.explained_variance_ratio_.sum())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.2860997781448586\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urKQ2lmZnkHu",
        "outputId": "976efbe7-9d15-46dc-8cea-e2bce1bb8d45"
      },
      "source": [
        "print(svd.singular_values_)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.55360944 1.5121377  1.51052009 1.37056529 1.19917045]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atY_JrTJqjh4"
      },
      "source": [
        "来看看我们具体的输出值是怎样的"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Sk1XdQCnpQ4",
        "outputId": "2f43ee77-c46c-4c03-b477-d253eb21f61f"
      },
      "source": [
        "svd.fit_transform(X).shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnH2h51wpCjl",
        "outputId": "2069c1eb-4968-4880-a6a3-0d1aa4a28bb3"
      },
      "source": [
        "svd.fit_transform(X)[:2]"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-5.07703085e-17, -1.98918547e-16,  5.27579738e-17,\n",
              "        -3.73171500e-16,  5.45109431e-18],\n",
              "       [-2.50932551e-06,  7.11886244e-03, -4.80647414e-06,\n",
              "        -3.06570234e-05, -3.15999399e-04]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2wpfUf7pb09"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}