{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\n\n\nimport nltk\nfrom nltk import word_tokenize\nimport string\nfrom nltk.corpus import stopwords \n\n\n#nltk.download('punkt')\n#nltk.download('averaged_perceptron_tagger')\n#nltk.download('wordnet')","execution_count":81,"outputs":[{"output_type":"stream","text":"[]\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"Text = \"Compatibility of systems of linear constraints over the set of natural numbers. \\\nCriteria of compatibility of a system of linear Diophantine equations, strict inequations, and \\\nnonstrict inequations are considered. \\\nUpper bounds for components of a minimal set of solutions and \\\nalgorithms of construction of minimal generating sets of solutions for all \\\ntypes of systems are given. \\\nThese criteria and the corresponding algorithms for constructing \\\na minimal supporting set of solutions can be used in solving all the \\\nconsidered types of systems and systems of mixed types.\"","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Text","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"'Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types of systems and systems of mixed types.'"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Clean text data (remove non-printable characters (if any) and turn words into lower case)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean(text):\n    text = text.lower()\n    printable = set(string.printable)\n    text1 = list(filter(lambda x: x in printable, text)) #filter funny characters, if any.\n    \n    # need to concatenate all strings in the filtered list \n    text2 = ''.join(text1)\n    return text2\n\nCleaned_text = clean(Text)\n\n\ntext = word_tokenize(Cleaned_text)\n\nprint (\"Tokenized Text: \\n\")\nprint (text)","execution_count":41,"outputs":[{"output_type":"stream","text":"Tokenized Text: \n\n['compatibility', 'of', 'systems', 'of', 'linear', 'constraints', 'over', 'the', 'set', 'of', 'natural', 'numbers', '.', 'criteria', 'of', 'compatibility', 'of', 'a', 'system', 'of', 'linear', 'diophantine', 'equations', ',', 'strict', 'inequations', ',', 'and', 'nonstrict', 'inequations', 'are', 'considered', '.', 'upper', 'bounds', 'for', 'components', 'of', 'a', 'minimal', 'set', 'of', 'solutions', 'and', 'algorithms', 'of', 'construction', 'of', 'minimal', 'generating', 'sets', 'of', 'solutions', 'for', 'all', 'types', 'of', 'systems', 'are', 'given', '.', 'these', 'criteria', 'and', 'the', 'corresponding', 'algorithms', 'for', 'constructing', 'a', 'minimal', 'supporting', 'set', 'of', 'solutions', 'can', 'be', 'used', 'in', 'solving', 'all', 'the', 'considered', 'types', 'of', 'systems', 'and', 'systems', 'of', 'mixed', 'types', '.']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(set(string.printable))","execution_count":79,"outputs":[{"output_type":"stream","text":"{'m', 'S', 'v', ')', '1', 'j', '{', '%', 'n', '?', '<', '>', 'f', '-', 'F', 'k', 'u', 'A', 'd', '/', '[', '.', '`', 'g', 'r', 'e', 'x', 'Q', '7', 'a', '8', '\\x0b', '*', 'G', 'J', '4', '}', '0', '@', 'L', 'W', 'M', 'p', ':', '6', 'z', '\\r', 'V', '!', 'q', 's', \"'\", 'E', '\\t', '\\n', 'O', 'U', 'I', '=', 'H', '(', 'o', '\"', 'i', 'y', ' ', '\\\\', '#', 'P', ',', 't', '$', ';', 'h', '9', '^', '_', 'Y', 'l', ']', 'K', '+', '3', '|', '5', 'b', 'w', '~', 'B', 'Z', 'N', '2', '&', 'C', 'X', 'c', '\\x0c', 'T', 'D', 'R'}\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# POS Tagging For Lemmatization"},{"metadata":{},"cell_type":"markdown","source":"POS tag list: https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/"},{"metadata":{"trusted":true},"cell_type":"code","source":"POS_tag = nltk.pos_tag(text)\n\nprint (\"Tokenized Text with POS tags: \\n\")\nprint (POS_tag)","execution_count":44,"outputs":[{"output_type":"stream","text":"[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n[nltk_data]     [Errno -3] Temporary failure in name resolution>\nTokenized Text with POS tags: \n\n[('compatibility', 'NN'), ('of', 'IN'), ('systems', 'NNS'), ('of', 'IN'), ('linear', 'JJ'), ('constraints', 'NNS'), ('over', 'IN'), ('the', 'DT'), ('set', 'NN'), ('of', 'IN'), ('natural', 'JJ'), ('numbers', 'NNS'), ('.', '.'), ('criteria', 'NNS'), ('of', 'IN'), ('compatibility', 'NN'), ('of', 'IN'), ('a', 'DT'), ('system', 'NN'), ('of', 'IN'), ('linear', 'JJ'), ('diophantine', 'NN'), ('equations', 'NNS'), (',', ','), ('strict', 'JJ'), ('inequations', 'NNS'), (',', ','), ('and', 'CC'), ('nonstrict', 'JJ'), ('inequations', 'NNS'), ('are', 'VBP'), ('considered', 'VBN'), ('.', '.'), ('upper', 'JJ'), ('bounds', 'NNS'), ('for', 'IN'), ('components', 'NNS'), ('of', 'IN'), ('a', 'DT'), ('minimal', 'JJ'), ('set', 'NN'), ('of', 'IN'), ('solutions', 'NNS'), ('and', 'CC'), ('algorithms', 'NN'), ('of', 'IN'), ('construction', 'NN'), ('of', 'IN'), ('minimal', 'JJ'), ('generating', 'VBG'), ('sets', 'NNS'), ('of', 'IN'), ('solutions', 'NNS'), ('for', 'IN'), ('all', 'DT'), ('types', 'NNS'), ('of', 'IN'), ('systems', 'NNS'), ('are', 'VBP'), ('given', 'VBN'), ('.', '.'), ('these', 'DT'), ('criteria', 'NNS'), ('and', 'CC'), ('the', 'DT'), ('corresponding', 'JJ'), ('algorithms', 'NN'), ('for', 'IN'), ('constructing', 'VBG'), ('a', 'DT'), ('minimal', 'JJ'), ('supporting', 'NN'), ('set', 'NN'), ('of', 'IN'), ('solutions', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('in', 'IN'), ('solving', 'VBG'), ('all', 'PDT'), ('the', 'DT'), ('considered', 'VBN'), ('types', 'NNS'), ('of', 'IN'), ('systems', 'NNS'), ('and', 'CC'), ('systems', 'NNS'), ('of', 'IN'), ('mixed', 'JJ'), ('types', 'NNS'), ('.', '.')]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Lemmatization\n\n### The tokenized text (mainly the nouns and adjectives) is normalized by lemmatization. In lemmatization different grammatical counterparts of a word will be replaced by single basic lemma. For example, 'glasses' may be replaced by 'glass'."},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\n\nwordnet_lemmatizer = WordNetLemmatizer()\n\nadjective_tags = ['JJ','JJR','JJS']\n\nlemmatized_text = []\n\nfor word in POS_tag:\n    if word[1] in adjective_tags:\n        lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0],pos=\"a\")))\n    else:\n        lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0]))) #default POS = noun\n        \nprint (\"Text tokens after lemmatization of adjectives and nouns: \\n\")\nprint (lemmatized_text)","execution_count":46,"outputs":[{"output_type":"stream","text":"Text tokens after lemmatization of adjectives and nouns: \n\n['compatibility', 'of', 'system', 'of', 'linear', 'constraint', 'over', 'the', 'set', 'of', 'natural', 'number', '.', 'criterion', 'of', 'compatibility', 'of', 'a', 'system', 'of', 'linear', 'diophantine', 'equation', ',', 'strict', 'inequations', ',', 'and', 'nonstrict', 'inequations', 'are', 'considered', '.', 'upper', 'bound', 'for', 'component', 'of', 'a', 'minimal', 'set', 'of', 'solution', 'and', 'algorithm', 'of', 'construction', 'of', 'minimal', 'generating', 'set', 'of', 'solution', 'for', 'all', 'type', 'of', 'system', 'are', 'given', '.', 'these', 'criterion', 'and', 'the', 'corresponding', 'algorithm', 'for', 'constructing', 'a', 'minimal', 'supporting', 'set', 'of', 'solution', 'can', 'be', 'used', 'in', 'solving', 'all', 'the', 'considered', 'type', 'of', 'system', 'and', 'system', 'of', 'mixed', 'type', '.']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"###  POS tagging again for Filtering"},{"metadata":{"trusted":true},"cell_type":"code","source":"POS_tag = nltk.pos_tag(lemmatized_text)\n\nprint (\"Lemmatized text with POS tags: \\n\")\nprint (POS_tag)","execution_count":47,"outputs":[{"output_type":"stream","text":"Lemmatized text with POS tags: \n\n[('compatibility', 'NN'), ('of', 'IN'), ('system', 'NN'), ('of', 'IN'), ('linear', 'JJ'), ('constraint', 'NN'), ('over', 'IN'), ('the', 'DT'), ('set', 'NN'), ('of', 'IN'), ('natural', 'JJ'), ('number', 'NN'), ('.', '.'), ('criterion', 'NN'), ('of', 'IN'), ('compatibility', 'NN'), ('of', 'IN'), ('a', 'DT'), ('system', 'NN'), ('of', 'IN'), ('linear', 'JJ'), ('diophantine', 'JJ'), ('equation', 'NN'), (',', ','), ('strict', 'JJ'), ('inequations', 'NNS'), (',', ','), ('and', 'CC'), ('nonstrict', 'JJ'), ('inequations', 'NNS'), ('are', 'VBP'), ('considered', 'VBN'), ('.', '.'), ('upper', 'JJ'), ('bound', 'NN'), ('for', 'IN'), ('component', 'NN'), ('of', 'IN'), ('a', 'DT'), ('minimal', 'JJ'), ('set', 'NN'), ('of', 'IN'), ('solution', 'NN'), ('and', 'CC'), ('algorithm', 'NN'), ('of', 'IN'), ('construction', 'NN'), ('of', 'IN'), ('minimal', 'JJ'), ('generating', 'VBG'), ('set', 'NN'), ('of', 'IN'), ('solution', 'NN'), ('for', 'IN'), ('all', 'DT'), ('type', 'NN'), ('of', 'IN'), ('system', 'NN'), ('are', 'VBP'), ('given', 'VBN'), ('.', '.'), ('these', 'DT'), ('criterion', 'NN'), ('and', 'CC'), ('the', 'DT'), ('corresponding', 'JJ'), ('algorithm', 'NN'), ('for', 'IN'), ('constructing', 'VBG'), ('a', 'DT'), ('minimal', 'JJ'), ('supporting', 'NN'), ('set', 'NN'), ('of', 'IN'), ('solution', 'NN'), ('can', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('in', 'IN'), ('solving', 'VBG'), ('all', 'PDT'), ('the', 'DT'), ('considered', 'VBN'), ('type', 'NN'), ('of', 'IN'), ('system', 'NN'), ('and', 'CC'), ('system', 'NN'), ('of', 'IN'), ('mixed', 'JJ'), ('type', 'NN'), ('.', '.')]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## POS Based Filtering\n\n### Any word from the lemmatized text, which isn't a noun, adjective, or gerund (or a 'foreign word'), is here considered as a stopword (non-content). This is based on the assumption that usually keywords are noun, adjectives or gerunds.  \n\n### Punctuations are added to the stopword list too."},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = []\n\n# NN\tnoun, singular 'desk'\n# NNS\tnoun plural\t'desks'\n# NNP\tproper noun, singular\t'Harrison'\n# NNPS\tproper noun, plural\t'Americans'\n# JJ\tadjective\t'big'\n# JJR\tadjective, comparative\t'bigger'\n# JJS\tadjective, superlative\t'biggest'\n# VBG\tverb, gerund/present participle\ttaking\n# FW\tforeign word\nwanted_POS = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS','VBG','FW'] \n\nfor word in POS_tag:\n    if word[1] not in wanted_POS:\n        stopwords.append(word[0])\n\npunctuations = list(str(string.punctuation))\n\nstopwords = stopwords + punctuations","execution_count":80,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words('english')) ","execution_count":82,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = list(stop_words) + punctuations","execution_count":51,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing Stopwords\n#### Removing stopwords from lemmatized_text. Processeced_text condtains the result."},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_text = []\nfor word in lemmatized_text:\n    if word not in stopwords:\n        processed_text.append(word)\nprint (processed_text)","execution_count":55,"outputs":[{"output_type":"stream","text":"['compatibility', 'system', 'linear', 'constraint', 'set', 'natural', 'number', 'criterion', 'compatibility', 'system', 'linear', 'diophantine', 'equation', 'strict', 'inequations', 'nonstrict', 'inequations', 'considered', 'upper', 'bound', 'component', 'minimal', 'set', 'solution', 'algorithm', 'construction', 'minimal', 'generating', 'set', 'solution', 'type', 'system', 'given', 'criterion', 'corresponding', 'algorithm', 'constructing', 'minimal', 'supporting', 'set', 'solution', 'used', 'solving', 'considered', 'type', 'system', 'system', 'mixed', 'type']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Vocabulary Creation\n#### Vocabulary will only contain unique words from processed_text."},{"metadata":{"trusted":true},"cell_type":"code","source":"vocabulary = list(set(processed_text))\nprint (vocabulary)","execution_count":56,"outputs":[{"output_type":"stream","text":"['solving', 'compatibility', 'supporting', 'used', 'natural', 'minimal', 'number', 'considered', 'generating', 'algorithm', 'bound', 'corresponding', 'constructing', 'strict', 'construction', 'system', 'equation', 'inequations', 'mixed', 'nonstrict', 'upper', 'component', 'type', 'given', 'diophantine', 'set', 'criterion', 'constraint', 'solution', 'linear']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Building Graph\n\nTextRank is a graph based model, and thus it requires us to build a graph. Each words in the vocabulary will serve as a vertex for graph. The words will be represented in the vertices by their index in vocabulary list.\n\nThe weighetd_edge matrix contains the information of edge connections among all vertices. I am building a graph with wieghted undirected edges.\n\nweighted_edge[i][j] contains the weight of the connecting edge between the word vertex represented by vocabulary index i and the word vertex represented by vocabulary j.\n\nIf weighted_edge[i][j] is zero, it means no edge or connection is present between the words represented by index i and j.\n\nThere is a connection between the words (and thus between i and j which represents them) if the words co-occur within a window of a specified 'window_size' in the processed_text.\n\nI am increasing value of the weighted_edge[i][j] is increased by (1/(distance between positions of words currently represented by i and j)) for every connection discovered between the same words in different locations of the text.\n\nThe covered_coocurrences list (which is contain the list of pairs of absolute positions in processed_text of the words whose coocurrence at that location is already checked) is managed so that the same two words located in the same positions in processed_text are not repetitively counted while sliding the window one text unit at a time.\n\nThe score of all vertices are intialized to one.\n\nSelf-connections are not considered, so weighted_edge[i][i] will be zero."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport math\nvocab_len = len(vocabulary)\n\nweighted_edge = np.zeros((vocab_len,vocab_len),dtype=np.float32)\n\nscore = np.zeros((vocab_len),dtype=np.float32)\nwindow_size = 3\ncovered_coocurrences = []\n\nfor i in range(0,vocab_len):\n    score[i]=1\n    for j in range(0,vocab_len):\n        if j==i:\n            weighted_edge[i][j]=0\n        else:\n            for window_start in range(0,(len(processed_text)-window_size+1)):\n                \n                window_end = window_start+window_size\n                \n                window = processed_text[window_start:window_end]\n                \n                if (vocabulary[i] in window) and (vocabulary[j] in window):\n                    \n                    index_of_i = window_start + window.index(vocabulary[i])\n                    index_of_j = window_start + window.index(vocabulary[j])\n                    \n                    # index_of_x is the absolute position of the xth term in the window \n                    # (counting from 0) \n                    # in the processed_text\n                      \n                    if [index_of_i,index_of_j] not in covered_coocurrences:\n                        weighted_edge[i][j]+=1/math.fabs(index_of_i-index_of_j)\n                        covered_coocurrences.append([index_of_i,index_of_j])","execution_count":58,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Calculating weighted summation of connections of a vertex  \n### inout[i] will contain the total no. of undirected connections\\edges associated withe the vertex represented by i."},{"metadata":{"trusted":true},"cell_type":"code","source":"inout = np.zeros((vocab_len),dtype=np.float32)\n\nfor i in range(0,vocab_len):\n    for j in range(0,vocab_len):\n        inout[i]+=weighted_edge[i][j]","execution_count":60,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scoring Vertices\n#### The formula used for scoring a vertex represented by i is:\n\n#### score[i] = (1-d) + d x [ Summation(j) ( (weighted_edge[i][j]/inout[j]) x score[j] ) ] where j belongs to the list of vertices that has a connection with i.\n\n#### d is the damping factor.\n\n#### The score is iteratively updated until convergence."},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_ITERATIONS = 50\nd=0.85\nthreshold = 0.0001 #convergence threshold\n\nfor iter in range(0,MAX_ITERATIONS):\n    prev_score = np.copy(score)\n    \n    for i in range(0,vocab_len):\n        \n        summation = 0\n        for j in range(0,vocab_len):\n            if weighted_edge[i][j] != 0:\n                summation += (weighted_edge[i][j]/inout[j])*score[j]\n                \n        score[i] = (1-d) + d*(summation)\n    \n    if np.sum(np.fabs(prev_score-score)) <= threshold: #convergence condition\n        print (\"Converging at iteration \"+str(iter)+\"....\")\n        break","execution_count":65,"outputs":[{"output_type":"stream","text":"Converging at iteration 21....\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0,vocab_len):\n    print (\"Score of \"+vocabulary[i]+\": \"+str(score[i]))","execution_count":67,"outputs":[{"output_type":"stream","text":"Score of solving: 0.69654167\nScore of compatibility: 0.9272843\nScore of supporting: 0.6604579\nScore of used: 0.6848075\nScore of natural: 0.6921799\nScore of minimal: 1.7975488\nScore of number: 0.6929364\nScore of considered: 1.3270594\nScore of generating: 0.6579498\nScore of algorithm: 1.2333438\nScore of bound: 0.7464653\nScore of corresponding: 0.69539475\nScore of constructing: 0.68657136\nScore of strict: 0.78876644\nScore of construction: 0.6688626\nScore of system: 2.2971506\nScore of equation: 0.7734541\nScore of inequations: 1.2132956\nScore of mixed: 0.5607787\nScore of nonstrict: 0.76826245\nScore of upper: 0.75452805\nScore of component: 0.7192911\nScore of type: 1.3728807\nScore of given: 0.6726919\nScore of diophantine: 0.73782545\nScore of set: 2.2908113\nScore of criterion: 1.2572458\nScore of constraint: 0.66726285\nScore of solution: 1.7187052\nScore of linear: 1.2393973\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Phrase Partitioning\n### Paritioning lemmatized_text into phrases using the stopwords in it as delimeters. The phrases are also candidates for keyphrases to be extracted."},{"metadata":{"trusted":true},"cell_type":"code","source":"phrases = []\n\nphrase = \" \"\nfor word in lemmatized_text:\n    \n    if word in stopwords:\n        if phrase!= \" \":\n            phrases.append(str(phrase).strip().split())\n        phrase = \" \"\n    elif word not in stopwords:\n        phrase+=str(word)\n        phrase+=\" \"\n\nprint (\"Partitioned Phrases (Candidate Keyphrases): \\n\")\nprint (phrases)","execution_count":70,"outputs":[{"output_type":"stream","text":"Partitioned Phrases (Candidate Keyphrases): \n\n[['compatibility'], ['system'], ['linear', 'constraint'], ['set'], ['natural', 'number'], ['criterion'], ['compatibility'], ['system'], ['linear', 'diophantine', 'equation'], ['strict', 'inequations'], ['nonstrict', 'inequations'], ['considered'], ['upper', 'bound'], ['component'], ['minimal', 'set'], ['solution'], ['algorithm'], ['construction'], ['minimal', 'generating', 'set'], ['solution'], ['type'], ['system'], ['given'], ['criterion'], ['corresponding', 'algorithm'], ['constructing'], ['minimal', 'supporting', 'set'], ['solution'], ['used'], ['solving'], ['considered', 'type'], ['system'], ['system'], ['mixed', 'type']]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Create a list of unique phrases.\n### Repeating phrases\\keyphrase candidates has no purpose here, anymore."},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_phrases = []\n\nfor phrase in phrases:\n    if phrase not in unique_phrases:\n        unique_phrases.append(phrase)\n\nprint (\"Unique Phrases (Candidate Keyphrases): \\n\")\nprint (unique_phrases)","execution_count":71,"outputs":[{"output_type":"stream","text":"Unique Phrases (Candidate Keyphrases): \n\n[['compatibility'], ['system'], ['linear', 'constraint'], ['set'], ['natural', 'number'], ['criterion'], ['linear', 'diophantine', 'equation'], ['strict', 'inequations'], ['nonstrict', 'inequations'], ['considered'], ['upper', 'bound'], ['component'], ['minimal', 'set'], ['solution'], ['algorithm'], ['construction'], ['minimal', 'generating', 'set'], ['type'], ['given'], ['corresponding', 'algorithm'], ['constructing'], ['minimal', 'supporting', 'set'], ['used'], ['solving'], ['considered', 'type'], ['mixed', 'type']]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Thinning the list of candidate-keyphrases.\n### Removing single word keyphrase-candidates that are present multi-word alternatives."},{"metadata":{"trusted":true},"cell_type":"code","source":"for word in vocabulary:\n    #print word\n    for phrase in unique_phrases:\n        if (word in phrase) and ([word] in unique_phrases) and (len(phrase)>1):\n            #if len(phrase)>1 then the current phrase is multi-worded.\n            #if the word in vocabulary is present in unique_phrases as a single-word-phrase\n            # and at the same time present as a word within a multi-worded phrase,\n            # then I will remove the single-word-phrase from the list.\n            unique_phrases.remove([word])\n            \nprint (\"Thinned Unique Phrases (Candidate Keyphrases): \\n\")\nprint (unique_phrases)","execution_count":72,"outputs":[{"output_type":"stream","text":"Thinned Unique Phrases (Candidate Keyphrases): \n\n[['compatibility'], ['system'], ['linear', 'constraint'], ['natural', 'number'], ['criterion'], ['linear', 'diophantine', 'equation'], ['strict', 'inequations'], ['nonstrict', 'inequations'], ['upper', 'bound'], ['component'], ['minimal', 'set'], ['solution'], ['construction'], ['minimal', 'generating', 'set'], ['given'], ['corresponding', 'algorithm'], ['constructing'], ['minimal', 'supporting', 'set'], ['used'], ['solving'], ['considered', 'type'], ['mixed', 'type']]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Scoring Keyphrases\n### Scoring the phrases (candidate keyphrases) and building up a list of keyphrases by listing untokenized versions of tokenized phrases\\candidate-keyphrases. Phrases are scored by adding the score of their members (words\\text-units that were ranked by the graph algorithm)"},{"metadata":{"trusted":true},"cell_type":"code","source":"phrase_scores = []\nkeywords = []\nfor phrase in unique_phrases:\n    phrase_score=0\n    keyword = ''\n    for word in phrase:\n        keyword += str(word)\n        keyword += \" \"\n        phrase_score+=score[vocabulary.index(word)]\n    phrase_scores.append(phrase_score)\n    keywords.append(keyword.strip())\n\ni=0\nfor keyword in keywords:\n    print (\"Keyword: '\"+str(keyword)+\"', Score: \"+str(phrase_scores[i]))\n    i+=1","execution_count":73,"outputs":[{"output_type":"stream","text":"Keyword: 'compatibility', Score: 0.927284300327301\nKeyword: 'system', Score: 2.2971506118774414\nKeyword: 'linear constraint', Score: 1.9066601395606995\nKeyword: 'natural number', Score: 1.3851163387298584\nKeyword: 'criterion', Score: 1.2572457790374756\nKeyword: 'linear diophantine equation', Score: 2.7506768703460693\nKeyword: 'strict inequations', Score: 2.0020620226860046\nKeyword: 'nonstrict inequations', Score: 1.9815580248832703\nKeyword: 'upper bound', Score: 1.5009933710098267\nKeyword: 'component', Score: 0.719291090965271\nKeyword: 'minimal set', Score: 4.088360071182251\nKeyword: 'solution', Score: 1.718705177307129\nKeyword: 'construction', Score: 0.6688625812530518\nKeyword: 'minimal generating set', Score: 4.746309876441956\nKeyword: 'given', Score: 0.6726918816566467\nKeyword: 'corresponding algorithm', Score: 1.9287385940551758\nKeyword: 'constructing', Score: 0.6865713596343994\nKeyword: 'minimal supporting set', Score: 4.748817980289459\nKeyword: 'used', Score: 0.6848074793815613\nKeyword: 'solving', Score: 0.6965416669845581\nKeyword: 'considered type', Score: 2.6999400854110718\nKeyword: 'mixed type', Score: 1.9336593747138977\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Ranking Keyphrases\n### Ranking keyphrases based on their calculated scores. Displaying top 'keywords_num' no. of keyphrases."},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_index = np.flip(np.argsort(phrase_scores),0)\n\nkeywords_num = 10\n\nprint (\"Keywords:\\n\")\n\nfor i in range(0,keywords_num):\n    print (str(keywords[sorted_index[i]])+\", \",)","execution_count":77,"outputs":[{"output_type":"stream","text":"Keywords:\n\nminimal supporting set, \nminimal generating set, \nminimal set, \nlinear diophantine equation, \nconsidered type, \nsystem, \nstrict inequations, \nnonstrict inequations, \nmixed type, \ncorresponding algorithm, \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary\n\n\n### Input text:\nCompatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types of systems and systems of mixed types.\n\n\n### Extracted Keywords:\nminimal supporting set,  \nminimal generating set,  \nminimal set,  \nlinear diophantine equation,  \nnonstrict inequations,  \nstrict inequations,  \nsystem,  \nlinear constraint,  \nsolution,  \nupper bound,"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}